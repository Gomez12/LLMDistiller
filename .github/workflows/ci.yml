name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  PYTHON_DEFAULT_VERSION: "3.11"

jobs:
  # Code Quality and Linting
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v6

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run Black formatting check
      run: black --check --diff src/ tests/

    - name: Run isort import sorting check
      run: isort --check-only --diff src/ tests/

    - name: Run flake8 linting
      run: flake8 src/ tests/ --max-line-length=88 --extend-ignore=E203,W503

    - name: Run mypy type checking
      run: mypy src/ --ignore-missing-imports

    - name: Check for complex code
      run: |
        pip install radon
        radon cc src/ --min B
        radon mi src/ --min B

  # Security Scanning
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v6

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install bandit safety

    - name: Run Bandit security scan
      run: bandit -r src/ -f json -o bandit-report.json
      continue-on-error: true

    - name: Run Safety dependency check
      run: safety check --json --output safety-report.json
      continue-on-error: true

    - name: Upload security reports
      uses: actions/upload-artifact@v4
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Testing Matrix
  test:
    name: Test Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.9", "3.10", "3.11"]
        database: ["sqlite", "postgresql"]
        exclude:
          # Only run PostgreSQL tests on Python 3.11 to save resources
          - python-version: "3.9"
            database: "postgresql"
          - python-version: "3.10"
            database: "postgresql"

    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
          POSTGRES_USER: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    if: matrix.database == 'postgresql'

    steps:
    - name: Checkout code
      uses: actions/checkout@v6

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Install PostgreSQL driver
      if: matrix.database == 'postgresql'
      run: pip install psycopg2-binary

    - name: Create test configuration
      run: |
        mkdir -p config
        if [ "${{ matrix.database }}" = "postgresql" ]; then
          cat > config/test_config.json << EOF
        {
          "database": {
            "url": "postgresql://test:test@localhost:5432/test_db",
            "echo": false
          },
          "llm_providers": {
            "test_provider": {
              "type": "openai",
              "api_key": "test-key",
              "base_url": "http://localhost:9999/v1",
              "model": "test-model"
            }
          },
          "processing": {
            "batch_size": 1,
            "concurrent_requests": 1
          },
          "logging": {
            "level": "WARNING"
          }
        }
        EOF
        else
          cat > config/test_config.json << EOF
        {
          "database": {
            "url": "sqlite:///:memory:",
            "echo": false
          },
          "llm_providers": {
            "test_provider": {
              "type": "openai",
              "api_key": "test-key",
              "base_url": "http://localhost:9999/v1",
              "model": "test-model"
            }
          },
          "processing": {
            "batch_size": 1,
            "concurrent_requests": 1
          },
          "logging": {
            "level": "WARNING"
          }
        }
        EOF
        fi

    - name: Wait for PostgreSQL
      if: matrix.database == 'postgresql'
      run: |
        timeout 60 bash -c 'until pg_isready -h localhost -p 5432; do sleep 1; done'

    - name: Initialize database
      run: |
        export PYTHONPATH=$PWD/src
        python -m src.main init --config config/test_config.json

    - name: Run unit tests
      run: |
        export PYTHONPATH=$PWD/src
        pytest tests/unit/ -v \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --junit-xml=unit-test-results.xml

    - name: Run integration tests
      run: |
        export PYTHONPATH=$PWD/src
        pytest tests/integration/ -v \
          --junit-xml=integration-test-results.xml

    - name: Upload coverage to Codecov
      if: matrix.python-version == env.PYTHON_DEFAULT_VERSION && matrix.database == 'sqlite'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}-${{ matrix.database }}
        path: |
          unit-test-results.xml
          integration-test-results.xml
          htmlcov/

  # CLI Testing
  cli-test:
    name: CLI Integration Test
    runs-on: ubuntu-latest
    needs: [quality, test]
    steps:
    - name: Checkout code
      uses: actions/checkout@v6

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e .

    - name: Test CLI commands
      run: |
        # Test help command
        llm-distiller --help
        
        # Test init command
        llm-distiller init --config-path ./test_config
        
        # Test status command
        llm-distiller status --config ./test_config/config.json || true
        
        # Test import with sample data
        llm-distiller import csv sample_questions.csv --config ./test_config/config.json || true

  # Build and Package
  build:
    name: Build Package
    runs-on: ubuntu-latest
    needs: [quality, test]
    steps:
    - name: Checkout code
      uses: actions/checkout@v6

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
        cache: 'pip'

    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine

    - name: Build package
      run: python -m build

    - name: Check package
      run: twine check dist/*

    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: dist-packages
        path: dist/

  # Performance Tests (only on main branch)
  performance:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [test]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
    - name: Checkout code
      uses: actions/checkout@v6

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install pytest-benchmark psutil memory-profiler

    - name: Run performance tests
      run: |
        export PYTHONPATH=$PWD/src
        pytest tests/performance/ -v \
          --benchmark-json=benchmark.json \
          --benchmark-only || true

    - name: Store benchmark result
      uses: benchmark-action/github-action-benchmark@v1
      if: success()
      with:
        tool: 'pytest'
        output-file-path: benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        alert-threshold: '200%'
        fail-on-alert: true

  # Status Check
  status:
    name: CI Status
    runs-on: ubuntu-latest
    needs: [quality, security, test, cli-test, build]
    if: always()
    steps:
    - name: Check results
      run: |
        if [[ "${{ needs.quality.result }}" != "success" ]]; then
          echo "❌ Quality checks failed"
          exit 1
        fi
        if [[ "${{ needs.test.result }}" != "success" ]]; then
          echo "❌ Tests failed"
          exit 1
        fi
        if [[ "${{ needs.cli-test.result }}" != "success" ]]; then
          echo "❌ CLI tests failed"
          exit 1
        fi
        if [[ "${{ needs.build.result }}" != "success" ]]; then
          echo "❌ Build failed"
          exit 1
        fi
        echo "✅ All checks passed!"